scale_fill_viridis_c(name = "f(X, Y)") +  # Continuous scale for the surface
# Optimizer trajectories
geom_point(data = trajectories, aes(x = x, y = y, color = method, shape = method), size = 7, stroke = 0.5) +
geom_text_repel(data = trajectories, aes(x = x, y = y, label = label, color = method),
size = 6, box.padding = 0.5, point.padding = 0.5, segment.color = "grey50") +
scale_color_manual(values = colors, name = "Optimizer") +  # Discrete colors for optimizers
scale_shape_manual(values = shapes) +  # Discrete shapes for optimizers
# Labels and theme
labs(
title = "3D Optimization Trajectories",
subtitle = "Optimizer step-by-step descent to the global minimum",
x = "X",
y = "Y"
) +
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
plot.subtitle = element_text(size = 16),
legend.title = element_text(size = 14),
legend.text = element_text(size = 12),
axis.title = element_text(size = 14),
axis.text = element_text(size = 12)
) +
transition_states(step, wrap = FALSE)
# Save the animation as a GIF
animate(animated_plot, fps = 10, width = 800, height = 600, renderer = gifski_renderer("optimization_3d_fixed.gif"))
# Save the animation as a GIF
animate(animated_plot, fps = 3, width = 800, height = 600, renderer = gifski_renderer("optimization_3d_fixed.gif"))
# Save the animation as a GIF
animate(animated_plot, fps = 5, width = 800, height = 600, renderer = gifski_renderer("optimization_3d_fixed.gif"))
# Render GIF in Quarto
path <- "optimization_3d_fixed.gif" # Path to GIF
# Render GIF in Quarto
path <- "optimization_3d_fixed.gif" # Path to GIF
---
### To Render Correctly:
- Ensure your Quarto document uses a **web-compatible format**:
# Render GIF in Quarto
path <- "optimization_3d_fixed.gif" # Path to GIF
yaml
# Loading required packages
library(tidyverse)
library(tidytuesdayR)
library(broom)
library(kableExtra)
library(ggplot2)
library(gganimate)
library(dplyr)
library(tidyr)
library(viridis)
library(ggrepel)  # For repelling labels
# Define the function to read dataset from a URL
readUrl <- function(url) {
tryCatch(
expr = {
plastics <- readr::read_csv(url)
message("Your dataset has been downloaded and saved!")
return(plastics)
},
error = function(e) {
message("Error downloading the dataset: ", e$message)
return(NULL)
}
)
}
url <- "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-26/plastics.csv"
# Load the dataset
x <- seq(-5, 5, length.out = 100)
y <- seq(-5, 5, length.out = 100)
grid <- expand.grid(x = x, y = y)
grid$z <- grid$x^2 + grid$y^2
library(ggplot2)
ggplot(grid, aes(x = x, y = y, z = z)) +
geom_contour_filled() # +
# labs(title = "Quadratic Function: f(x, y) = x^2 + y^2")
a <- 1
b <- 100
x <- seq(-2, 2, length.out = 100)
y <- seq(-1, 3, length.out = 100)
grid <- expand.grid(x = x, y = y)
grid$z <- (a - grid$x)^2 + b * (grid$y - grid$x^2)^2
ggplot(grid, aes(x = x, y = y, z = z)) +
geom_contour_filled() # +
# labs(title = "Rosenbrock Function: f(x, y) = (a - x)^2 + b * (y - x^2)^2")
x <- seq(-5, 5, length.out = 100)
y <- seq(-5, 5, length.out = 100)
grid <- expand.grid(x = x, y = y)
grid$z <- grid$x^2 - grid$y^2
ggplot(grid, aes(x = x, y = y, z = z)) +
geom_contour_filled() # +
# labs(title = "Saddle Point Function: f(x, y) = x^2 - y^2")
# Gradient Descent on Quadratic Function
library(ggplot2)
# Gradient Descent Function
gradient_descent <- function(x_init, y_init, learning_rate, n_iter) {
x <- x_init
y <- y_init
# Initialize path with initial values for x, y, dx, and dy
path <- data.frame(x = x, y = y, dx = NA, dy = NA)
for (i in 1:n_iter) {
grad_x <- 2 * x  # Gradient with respect to x
grad_y <- 2 * y  # Gradient with respect to y
dx <- -learning_rate * grad_x
dy <- -learning_rate * grad_y
x <- x + dx
y <- y + dy
# Store the updated position and direction (dx, dy)
path <- rbind(path, data.frame(x = x, y = y, dx = dx, dy = dy))
}
return(path)
}
# Initial point and learning rate
x_init <- 4
y_init <- 4
learning_rate <- 0.1
n_iter <- 50
# Run gradient descent
path <- gradient_descent(x_init, y_init, learning_rate, n_iter)
# Plotting the trajectory with arrows to show direction
ggplot(path, aes(x = x, y = y)) +
geom_path() +
geom_point(aes(x = x_init, y = y_init), color = "red") +
geom_segment(aes(xend = x + dx, yend = y + dy),
arrow = arrow(type = "closed", length = unit(0.2, "inches")),
size = 0.5) +
labs(title = "Gradient Descent Trajectory on f(x, y) = x^2 + y^2") +
theme_minimal()
# Stochastic Gradient Descent on Quadratic Function
library(ggplot2)
# Stochastic Gradient Descent Function
stochastic_gradient_descent <- function(x_init, y_init, learning_rate, n_iter) {
x <- x_init
y <- y_init
# Initialize path with x, y, dx, dy columns for storing positions and direction
path <- data.frame(x = x, y = y, dx = NA, dy = NA)
for (i in 1:n_iter) {
grad_x <- 2 * x + runif(1, -0.1, 0.1)  # Add noise to gradient for x
grad_y <- 2 * y + runif(1, -0.1, 0.1)  # Add noise to gradient for y
dx <- -learning_rate * grad_x  # Update for x
dy <- -learning_rate * grad_y  # Update for y
x <- x + dx
y <- y + dy
# Append the updated position and direction
path <- rbind(path, data.frame(x = x, y = y, dx = dx, dy = dy))
}
return(path)
}
# Initial point and learning rate
x_init <- 4
y_init <- 4
learning_rate <- 0.1
n_iter <- 50
# Run stochastic gradient descent
path_sgd <- stochastic_gradient_descent(x_init, y_init, learning_rate, n_iter)
# Plotting the trajectory with arrows to show learning rate direction
ggplot(path_sgd, aes(x = x, y = y)) +
geom_path() +
geom_point(aes(x = x_init, y = y_init), color = "red") +
geom_segment(aes(xend = x + dx, yend = y + dy),
arrow = arrow(type = "closed", length = unit(0.2, "inches")),
size = 0.5) +  # Add arrows to show direction
labs(title = "SGD Trajectory on f(x, y) = x^2 + y^2 with Noise") +
theme_minimal()
# Momentum Optimization on Quadratic Function
library(ggplot2)
# Momentum Optimization Function
momentum_optimizer <- function(x_init, y_init, learning_rate, beta, n_iter) {
x <- x_init
y <- y_init
v_x <- 0  # Initialize velocity for x
v_y <- 0  # Initialize velocity for y
path <- data.frame(x = x, y = y, dx = NA, dy = NA)  # Track position and direction
for (i in 1:n_iter) {
grad_x <- 2 * x
grad_y <- 2 * y
# Update velocity
v_x <- beta * v_x + (1 - beta) * grad_x
v_y <- beta * v_y + (1 - beta) * grad_y
# Update parameters (position)
x <- x - learning_rate * v_x
y <- y - learning_rate * v_y
# Store the updated position and step direction (dx, dy)
path <- rbind(path, data.frame(x = x, y = y, dx = -learning_rate * v_x, dy = -learning_rate * v_y))
}
return(path)
}
# Run Momentum Optimization
path_momentum <- momentum_optimizer(x_init = 4, y_init = 4, learning_rate = 0.1, beta = 0.9, n_iter = 50)
# Plotting the trajectory with arrows to show direction of learning rate
ggplot(path_momentum, aes(x = x, y = y)) +
geom_path() +
geom_point(aes(x = 4, y = 4), color = "red") +  # Starting point (red)
geom_segment(aes(xend = x + dx, yend = y + dy),
arrow = arrow(type = "closed", length = unit(0.2, "inches")),
size = 0.5, color = "blue") +  # Arrows showing direction of learning rate
labs(title = "Momentum Optimizer with Learning Rate Direction on f(x, y) = x^2 + y^2") +
theme_minimal()
library(ggplot2)
# AdaGrad Optimization Function
adagrad_optimizer <- function(x_init, y_init, learning_rate, epsilon, n_iter) {
x <- x_init
y <- y_init
G_x <- 0  # Initialize sum of squared gradients for x
G_y <- 0  # Initialize sum of squared gradients for y
path <- data.frame(x = x, y = y, dx = NA, dy = NA)  # Track position and direction
for (i in 1:n_iter) {
grad_x <- 2 * x
grad_y <- 2 * y
# Update sum of squared gradients
G_x <- G_x + grad_x^2
G_y <- G_y + grad_y^2
# Update parameters (position)
x <- x - (learning_rate / (sqrt(G_x) + epsilon)) * grad_x
y <- y - (learning_rate / (sqrt(G_y) + epsilon)) * grad_y
# Store the updated position and the direction based on the adaptive learning rate
path <- rbind(path, data.frame(x = x, y = y,
dx = - (learning_rate / (sqrt(G_x) + epsilon)) * grad_x,
dy = - (learning_rate / (sqrt(G_y) + epsilon)) * grad_y))
}
return(path)
}
# Run AdaGrad Optimization
path_adagrad <- adagrad_optimizer(x_init = 4, y_init = 4, learning_rate = 0.1, epsilon = 1e-8, n_iter = 50)
# Plotting the trajectory with arrows to show direction of learning rate
ggplot(path_adagrad, aes(x = x, y = y)) +
geom_path() +
geom_point(aes(x = 4, y = 4), color = "red") +  # Starting point (red)
geom_segment(aes(xend = x + dx, yend = y + dy),
arrow = arrow(type = "closed", length = unit(0.2, "inches")),
size = 0.5, color = "blue") +  # Arrows showing direction of learning rate
labs(title = "AdaGrad Optimizer with Learning Rate Direction on f(x, y) = x^2 + y^2") +
theme_minimal()
# RMSProp Optimization on Quadratic Function
library(ggplot2)
# RMSProp Optimization Function
rmsprop_optimizer <- function(x_init, y_init, learning_rate, beta, epsilon, n_iter) {
x <- x_init
y <- y_init
v_x <- 0  # Initialize moving average for x
v_y <- 0  # Initialize moving average for y
path <- data.frame(x = x, y = y, dx = NA, dy = NA)  # Track position and direction
for (i in 1:n_iter) {
grad_x <- 2 * x
grad_y <- 2 * y
# Update moving averages of squared gradients
v_x <- beta * v_x + (1 - beta) * grad_x^2
v_y <- beta * v_y + (1 - beta) * grad_y^2
# Update parameters (position)
x <- x - (learning_rate / (sqrt(v_x) + epsilon)) * grad_x
y <- y - (learning_rate / (sqrt(v_y) + epsilon)) * grad_y
# Store the updated position and direction based on the adaptive learning rate
path <- rbind(path, data.frame(x = x, y = y,
dx = - (learning_rate / (sqrt(v_x) + epsilon)) * grad_x,
dy = - (learning_rate / (sqrt(v_y) + epsilon)) * grad_y))
}
return(path)
}
# Run RMSProp Optimization
path_rmsprop <- rmsprop_optimizer(x_init = 4, y_init = 4, learning_rate = 0.1, beta = 0.9, epsilon = 1e-8, n_iter = 50)
# Plotting the trajectory with arrows to show direction of learning rate
ggplot(path_rmsprop, aes(x = x, y = y)) +
geom_path() +
geom_point(aes(x = 4, y = 4), color = "red") +  # Starting point (red)
geom_segment(aes(xend = x + dx, yend = y + dy),
arrow = arrow(type = "closed", length = unit(0.2, "inches")),
size = 0.5, color = "blue") +  # Arrows showing direction of learning rate
labs(title = "RMSProp Optimizer with Learning Rate Direction on f(x, y) = x^2 + y^2") +
theme_minimal()
library(ggplot2)
# Adam Optimization Function with Learning Rate Direction
adam_optimizer <- function(x_init, y_init, learning_rate, beta1, beta2, epsilon, n_iter) {
x <- x_init
y <- y_init
m_x <- 0  # Initialize first moment estimate for x
m_y <- 0  # Initialize first moment estimate for y
v_x <- 0  # Initialize second moment estimate for x
v_y <- 0  # Initialize second moment estimate for y
path <- data.frame(x = x, y = y, dx = NA, dy = NA)  # Track position and direction
for (i in 1:n_iter) {
grad_x <- 2 * x  # Gradient for x
grad_y <- 2 * y  # Gradient for y
# Update first moment estimate (mean of gradients)
m_x <- beta1 * m_x + (1 - beta1) * grad_x
m_y <- beta1 * m_y + (1 - beta1) * grad_y
# Update second moment estimate (variance of gradients)
v_x <- beta2 * v_x + (1 - beta2) * grad_x^2
v_y <- beta2 * v_y + (1 - beta2) * grad_y^2
# Bias correction for moment estimates
m_x_hat <- m_x / (1 - beta1^i)
m_y_hat <- m_y / (1 - beta1^i)
v_x_hat <- v_x / (1 - beta2^i)
v_y_hat <- v_y / (1 - beta2^i)
# Update parameters
dx <- (learning_rate / (sqrt(v_x_hat) + epsilon)) * m_x_hat
dy <- (learning_rate / (sqrt(v_y_hat) + epsilon)) * m_y_hat
x <- x - dx
y <- y - dy
# Save parameter updates and directions for plotting
path <- rbind(path, data.frame(x = x, y = y, dx = -dx, dy = -dy))
}
return(path)
}
# Run Adam Optimization
path_adam <- adam_optimizer(x_init = 4, y_init = 4, learning_rate = 0.1, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, n_iter = 50)
# Plotting the trajectory with arrows to show direction of learning rate
ggplot(path_adam, aes(x = x, y = y)) +
geom_path() +
geom_point(aes(x = 4, y = 4), color = "red") +  # Starting point (red)
geom_segment(aes(xend = x + dx, yend = y + dy),
arrow = arrow(type = "closed", length = unit(0.2, "inches")),
size = 0.5, color = "blue") +  # Arrows showing direction of learning rate
labs(title = "Adam Optimizer with Learning Rate Direction on f(x, y) = x^2 + y^2") +
theme_minimal()
# Load necessary libraries
library(ggplot2)
library(gganimate)
library(dplyr)
library(tidyr)
library(viridis)
library(ggrepel)  # For repelling labels
# Define the quadratic function
quadratic_function <- function(x, y) {
return(x^2 + y^2)
}
# Generate the 3D surface as a data frame
generate_surface <- function() {
x <- seq(-5, 5, length.out = 100)
y <- seq(-5, 5, length.out = 100)
expand.grid(x = x, y = y) %>%
mutate(z = quadratic_function(x, y))
}
surface <- generate_surface()
# Gradient Descent
gradient_descent <- function(x_init, y_init, learning_rate, n_iter) {
x <- x_init
y <- y_init
path <- data.frame(step = 0, x = x, y = y, z = quadratic_function(x, y), method = "GD")
for (i in 1:n_iter) {
grad <- c(2 * x, 2 * y)
x <- x - learning_rate * grad[1]
y <- y - learning_rate * grad[2]
path <- rbind(path, data.frame(step = i, x = x, y = y, z = quadratic_function(x, y), method = "GD"))
}
return(path)
}
# Stochastic Gradient Descent
stochastic_gradient_descent <- function(x_init, y_init, learning_rate, n_iter) {
x <- x_init
y <- y_init
path <- data.frame(step = 0, x = x, y = y, z = quadratic_function(x, y), method = "SGD")
for (i in 1:n_iter) {
grad <- c(2 * x, 2 * y) + runif(2, -0.1, 0.1)
x <- x - learning_rate * grad[1]
y <- y - learning_rate * grad[2]
path <- rbind(path, data.frame(step = i, x = x, y = y, z = quadratic_function(x, y), method = "SGD"))
}
return(path)
}
# Momentum
momentum_optimizer <- function(x_init, y_init, learning_rate, beta, n_iter) {
x <- x_init
y <- y_init
v_x <- 0
v_y <- 0
path <- data.frame(step = 0, x = x, y = y, z = quadratic_function(x, y), method = "Momentum")
for (i in 1:n_iter) {
grad <- c(2 * x, 2 * y)
v_x <- beta * v_x + (1 - beta) * grad[1]
v_y <- beta * v_y + (1 - beta) * grad[2]
x <- x - learning_rate * v_x
y <- y - learning_rate * v_y
path <- rbind(path, data.frame(step = i, x = x, y = y, z = quadratic_function(x, y), method = "Momentum"))
}
return(path)
}
# AdaGrad
adagrad_optimizer <- function(x_init, y_init, learning_rate, epsilon, n_iter) {
x <- x_init
y <- y_init
G_x <- 0
G_y <- 0
path <- data.frame(step = 0, x = x, y = y, z = quadratic_function(x, y), method = "AdaGrad")
for (i in 1:n_iter) {
grad <- c(2 * x, 2 * y)
G_x <- G_x + grad[1]^2
G_y <- G_y + grad[2]^2
x <- x - (learning_rate / (sqrt(G_x) + epsilon)) * grad[1]
y <- y - (learning_rate / (sqrt(G_y) + epsilon)) * grad[2]
path <- rbind(path, data.frame(step = i, x = x, y = y, z = quadratic_function(x, y), method = "AdaGrad"))
}
return(path)
}
# RMSProp
rmsprop_optimizer <- function(x_init, y_init, learning_rate, beta, epsilon, n_iter) {
x <- x_init
y <- y_init
v_x <- 0
v_y <- 0
path <- data.frame(step = 0, x = x, y = y, z = quadratic_function(x, y), method = "RMSProp")
for (i in 1:n_iter) {
grad <- c(2 * x, 2 * y)
v_x <- beta * v_x + (1 - beta) * grad[1]^2
v_y <- beta * v_y + (1 - beta) * grad[2]^2
x <- x - (learning_rate / (sqrt(v_x) + epsilon)) * grad[1]
y <- y - (learning_rate / (sqrt(v_y) + epsilon)) * grad[2]
path <- rbind(path, data.frame(step = i, x = x, y = y, z = quadratic_function(x, y), method = "RMSProp"))
}
return(path)
}
# Adam
adam_optimizer <- function(x_init, y_init, learning_rate, beta1, beta2, epsilon, n_iter) {
x <- x_init
y <- y_init
m_x <- 0
m_y <- 0
v_x <- 0
v_y <- 0
path <- data.frame(step = 0, x = x, y = y, z = quadratic_function(x, y), method = "Adam")
for (i in 1:n_iter) {
grad <- c(2 * x, 2 * y)
m_x <- beta1 * m_x + (1 - beta1) * grad[1]
m_y <- beta1 * m_y + (1 - beta1) * grad[2]
v_x <- beta2 * v_x + (1 - beta2) * grad[1]^2
v_y <- beta2 * v_y + (1 - beta2) * grad[2]^2
m_x_hat <- m_x / (1 - beta1^i)
m_y_hat <- m_y / (1 - beta1^i)
v_x_hat <- v_x / (1 - beta2^i)
v_y_hat <- v_y / (1 - beta2^i)
x <- x - (learning_rate / (sqrt(v_x_hat) + epsilon)) * m_x_hat
y <- y - (learning_rate / (sqrt(v_y_hat) + epsilon)) * m_y_hat
path <- rbind(path, data.frame(step = i, x = x, y = y, z = quadratic_function(x, y), method = "Adam"))
}
return(path)
}
# Generate paths for all optimizers
# Generate optimizer trajectories
n_iter <- 50
learning_rate <- 0.1
trajectories <- bind_rows(
gradient_descent(4, 4, learning_rate, n_iter),
stochastic_gradient_descent(4, 4, learning_rate, n_iter),
momentum_optimizer(4, 4, learning_rate, beta = 0.9, n_iter),
adagrad_optimizer(4, 4, learning_rate, epsilon = 1e-8, n_iter),
rmsprop_optimizer(4, 4, learning_rate, beta = 0.9, epsilon = 1e-8, n_iter),
adam_optimizer(4, 4, learning_rate, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, n_iter)
)
# Update labels to include method abbreviation
trajectories <- trajectories %>%
mutate(label = paste(method, step))
# Assign colors and shapes to optimizers
shapes <- c("GD" = 15, "SGD" = 17, "Momentum" = 18, "AdaGrad" = 19, "RMSProp" = 24, "Adam" = 25)
colors <- viridis::viridis(6)  # Define distinct colors for optimizers
# Create animated plot
animated_plot <- ggplot() +
# 3D surface
geom_tile(data = surface, aes(x = x, y = y, fill = z), alpha = 0.8) +
scale_fill_viridis_c(name = "f(X, Y)") +  # Continuous scale for the surface
# Optimizer trajectories
geom_point(data = trajectories, aes(x = x, y = y, color = method, shape = method), size = 7, stroke = 0.5) +
geom_text_repel(data = trajectories, aes(x = x, y = y, label = label, color = method),
size = 6, box.padding = 0.5, point.padding = 0.5, segment.color = "grey50") +
scale_color_manual(values = colors, name = "Optimizer") +  # Discrete colors for optimizers
scale_shape_manual(values = shapes) +  # Discrete shapes for optimizers
# Labels and theme
labs(
title = "3D Optimization Trajectories",
subtitle = "Optimizer step-by-step descent to the global minimum",
x = "X",
y = "Y"
) +
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
plot.subtitle = element_text(size = 16),
legend.title = element_text(size = 14),
legend.text = element_text(size = 12),
axis.title = element_text(size = 14),
axis.text = element_text(size = 12)
) +
transition_states(step, wrap = FALSE)
# Save the animation as a GIF
animate(animated_plot, fps = 5, width = 800, height = 600, renderer = gifski_renderer("optimization_3d_fixed.gif"))
# Create animated plot
animated_plot <- ggplot() +
# 3D surface
geom_tile(data = surface, aes(x = x, y = y, fill = z), alpha = 0.8) +
scale_fill_viridis_c(name = "f(X, Y)") +  # Continuous scale for the surface
# Optimizer trajectories
geom_point(data = trajectories, aes(x = x, y = y, color = method, shape = method), size = 7, stroke = 0.5) +
geom_text_repel(data = trajectories, aes(x = x, y = y, label = label, color = method),
size = 6, box.padding = 0.5, point.padding = 0.5, segment.color = "grey50") +
scale_color_manual(values = colors, name = "Optimizer") +  # Discrete colors for optimizers
scale_shape_manual(values = shapes) +  # Discrete shapes for optimizers
# Labels and theme
labs(
title = "Optimization Trajectories to find Global Minimum",
subtitle = "Deep learning optimizer dynamics",
x = "X",
y = "Y"
) +
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
plot.subtitle = element_text(size = 16),
legend.title = element_text(size = 14),
legend.text = element_text(size = 12),
axis.title = element_text(size = 14),
axis.text = element_text(size = 12)
) +
transition_states(step, wrap = FALSE)
# Save the animation as a GIF
animate(animated_plot, fps = 5, width = 800, height = 600, renderer = gifski_renderer("optimization_3d_fixed.gif"))
